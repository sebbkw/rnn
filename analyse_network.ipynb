{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up imports, load model and virtual physiology data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import scipy.optimize as opt\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import animation, rc\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "\n",
    "# Import custom modules\n",
    "from lib.FramesDataset import FramesDataset\n",
    "from lib import network\n",
    "\n",
    "# Import Luke's brainbox package for gabor fitting\n",
    "from brainbox import physiology\n",
    "\n",
    "#from gabor_fit import fit\n",
    "\n",
    "# Global variables\n",
    "HIDDEN_UNITS = 1600\n",
    "FRAME_SIZE = 20\n",
    "WARMUP = 4\n",
    "MODE = 'hierarchical'\n",
    "T_STEPS = 45\n",
    "\n",
    "# Global graph variables\n",
    "GROUP1_COLOR = 'dodgerblue'\n",
    "GROUP2_COLOR = 'tomato'\n",
    "NEUTRAL_COLOR = 'slategrey'\n",
    "\n",
    "PATHS = [\n",
    "    # 15px\n",
    "    'hierarchical-233450examples-15framesize-45tsteps-4warmup-2000epochs-1600units-0.0001lr-0.25gradclip-1e-06L1/0.5beta',\n",
    "    # 20px\n",
    "    'hierarchical-290290examples-20framesize-45tsteps-4warmup-2000epochs-1600units-0.0001lr-0.25gradclip-1e-6.25L1/0.2beta',\n",
    "    'hierarchical-290290examples-20framesize-45tsteps-4warmup-2000epochs-1600units-0.0001lr-0.25gradclip-1e-6.25L1/0.5beta',\n",
    "    # 20px, 1000 epochs\n",
    "    'model-hierarchical-290290examples-20framesize-45tsteps-4warmup-1000epochs-1600units-0.0001lr-0.25gradclip-1e-6.25L1/0.2beta',\n",
    "    # 20px, 3000 epochs\n",
    "    'hierarchical-290290examples-20framesize-45tsteps-4warmup-3000epochs-1600units-0.0001lr-0.25gradclip-1e-6.25L1/0.2beta'\n",
    "]\n",
    "PATH = PATHS[2]\n",
    "\n",
    "# Set device to use on network\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using\", DEVICE)\n",
    "\n",
    "# Load previously trained model\n",
    "model = network.RecurrentTemporalPrediction.load(\n",
    "    hidden_units = HIDDEN_UNITS,\n",
    "    frame_size = FRAME_SIZE,\n",
    "    warmup = WARMUP,\n",
    "    mode = MODE,\n",
    "    path = './models/' + PATH + '.pt'\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "if False:\n",
    "    train_dataset = FramesDataset('./datasets/processed_dataset_15px_20tsteps.npy', 'train', WARMUP)\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128)\n",
    "    print(\"Training dataset length:\", len(train_dataset))\n",
    "\n",
    "# Plot loss history\n",
    "with open('./models/' + PATH + '.pickle', 'rb') as p:\n",
    "    loss_history = pickle.load(p)\n",
    "    \n",
    "plt.loglog(loss_history);\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean loss')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or save previously generated physiology data\n",
    "\n",
    "mode = 'LOAD'\n",
    "path = './virtual_physiology/' + PATH + '.pickle'\n",
    "\n",
    "if mode == 'SAVE':\n",
    "    data = {\n",
    "        \"physiology_data_group1\": physiology_data_group1,\n",
    "        \"physiology_data_group2\": physiology_data_group2,\n",
    "        \"averaged_stimuli_group1\": averaged_stimuli_group1,\n",
    "        \"averaged_stimuli_group2\": averaged_stimuli_group2\n",
    "    }\n",
    "\n",
    "    with open(path, 'wb') as handler:\n",
    "        pickle.dump(data, handler)\n",
    "        print('Saved')\n",
    "elif mode == 'LOAD':\n",
    "    with open(path, 'rb') as handler:\n",
    "        data = pickle.load(handler)\n",
    "        \n",
    "        physiology_data_group1 = data[\"physiology_data_group1\"]\n",
    "        physiology_data_group2 = data[\"physiology_data_group2\"]\n",
    "        averaged_stimuli_group1 = data[\"averaged_stimuli_group1\"]\n",
    "        averaged_stimuli_group2 = data[\"averaged_stimuli_group2\"]\n",
    "        print('Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabor fitting\n",
    "\n",
    "gabor_path = './virtual_physiology/' + PATH + '_group1gabor.csv'\n",
    "spatiotemporal_rfs = torch.Tensor([\n",
    "    unit[\"rf_heatmap\"] for unit in physiology_data_group1\n",
    "])\n",
    "spatiotemporal_rfs = spatiotemporal_rfs.unsqueeze(1)\n",
    "\n",
    "physiology.fit_gabors(\n",
    "    path, '', spatiotemporal_rfs,\n",
    "    spatial_locations=None, sigmas=None, thetas=None,\n",
    "    phis=None, frequencies=None, n_spectral_itr=800,\n",
    "    n_spatial_itr=1200, spectral_lr=0.01, spatial_lr=0.002\n",
    ")\n",
    "\n",
    "gabor_path = './virtual_physiology/' + PATH + '_group2gabor.csv'\n",
    "spatiotemporal_rfs = torch.Tensor([\n",
    "    unit[\"rf_heatmap\"] for unit in physiology_data_group2\n",
    "])\n",
    "spatiotemporal_rfs = spatiotemporal_rfs.unsqueeze(1)\n",
    "\n",
    "physiology.fit_gabors(\n",
    "    path, '', spatiotemporal_rfs,\n",
    "    spatial_locations=None, sigmas=None, thetas=None,\n",
    "    phis=None, frequencies=None, n_spectral_itr=800,\n",
    "    n_spatial_itr=1200, spectral_lr=0.01, spatial_lr=0.002\n",
    ")\n",
    "\n",
    "#fit_params_df, gabor_fits, largest_power_rfs, spatiotemporal_rfs = physiology.get_gabors(\n",
    "#    path, '', spatiotemporal_rfs\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/JesseLivezey/gabor_fit/blob/master/notebooks/simple_usage.ipynb\n",
    "\n",
    "rfs = np.array([\n",
    "    unit[\"rf_heatmap\"] for unit in physiology_data_group1[:10]\n",
    "])\n",
    "rfs = np.moveaxis(rfs, 0, -1)\n",
    "\n",
    "gf = fit.GaborFit()\n",
    "_, pp, se = gf.fit(rfs)\n",
    "pp = fit.combine_params(*pp)\n",
    "\n",
    "gs = gf.make_gabor(pp, 15, 15)\n",
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(5, 2, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(gs[:, :, i])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "for i, unit in enumerate(physiology_data_group1[:10]):\n",
    "    ax = fig.add_subplot(5, 2, i+1, xticks=[], yticks=[])\n",
    "    response = unit[\"rf_heatmap\"] \n",
    "    ax.imshow(response)\n",
    "    \n",
    "    centre = unit[\"position\"]\n",
    "    if not np.isnan(centre[0]):\n",
    "        ax.scatter([ int(centre[0]*0.75) ], [ int(centre[1]*0.75) ], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weight matrices and weights distribution\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=plt.figaspect(1/3), dpi=150)\n",
    "\n",
    "axs[0].hist(model.rnn.weight_hh_l0.cpu().detach().numpy().flat, log=True)\n",
    "axs[0].set_xlabel('Weight')\n",
    "axs[0].set_ylabel('Log frequency')\n",
    "axs[0].set_title('H-H RNN weight distribution')\n",
    "\n",
    "im = axs[1].imshow(model.rnn.weight_hh_l0.cpu().detach().numpy(), vmin=-0.01, vmax=0.01)\n",
    "axs[1].set_xlabel('Hidden weights')\n",
    "axs[1].set_ylabel('Hidden weights')\n",
    "axs[1].set_title(\"H-H RNN weights\")\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process virtual physiology data and get RF locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_abs_normalize (arr):\n",
    "    return arr / np.max(np.abs(arr))\n",
    "\n",
    "def normalize (arr):\n",
    "    return np.array([\n",
    "        (lambda a: (a-np.mean(a)) / np.std(a))(b)\n",
    "        for b in arr\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Produce spike-triggered average\n",
    "n_rand_stimuli = 250\n",
    "\n",
    "stimuli = {}\n",
    "unit_activity = {}\n",
    "\n",
    "for i in range(HIDDEN_UNITS):\n",
    "    stimuli[i] = []\n",
    "    unit_activity[i] = []\n",
    "\n",
    "for i in range(n_rand_stimuli):\n",
    "    if i % 10 == 9:\n",
    "        print('Random noise trial {}'.format(i+1))\n",
    "    \n",
    "    noise_shape = (1, WARMUP+T_STEPS, FRAME_SIZE**2)\n",
    "    noise = np.random.normal(loc=0, scale=1, size=noise_shape)\n",
    "    noise = torch.Tensor(noise).to(DEVICE)\n",
    "    \n",
    "    _, hidden_state = model(noise)\n",
    "    \n",
    "    for t_step in range(WARMUP, WARMUP+T_STEPS):\n",
    "        units = hidden_state[0, t_step, :] # Output t_step\n",
    "        for i, unit in enumerate(units):\n",
    "            if unit > 0:\n",
    "                weighted_stimulus = noise[0, t_step, :] * unit\n",
    "                stimuli[i].append(weighted_stimulus.cpu().detach().numpy())\n",
    "                unit_activity[i].append(unit.cpu().detach().numpy())\n",
    "                \n",
    "averaged_stimuli_group1 = [], []\n",
    "averaged_stimuli_group2 = [], []\n",
    "\n",
    "for i in range(HIDDEN_UNITS):\n",
    "    if i % 100 == 99:\n",
    "        print('Averaging across hidden unit {}'.format(i+1))\n",
    "    \n",
    "    stimuli_len = len(stimuli[i])\n",
    "    if stimuli_len:\n",
    "        stimulus = np.sum(stimuli[i], 0)/np.sum(unit_activity[i]) # Weighted average\n",
    "\n",
    "        if i < HIDDEN_UNITS//2:\n",
    "            averaged_stimuli_group1[0].append(stimulus)\n",
    "            averaged_stimuli_group1[1].append(i)\n",
    "        else:\n",
    "            averaged_stimuli_group2[0].append(stimulus)\n",
    "            averaged_stimuli_group2[1].append(i)\n",
    "averaged_stimuli_group1 = np.array(averaged_stimuli_group1[0]), np.array(averaged_stimuli_group1[1])\n",
    "averaged_stimuli_group2 = np.array(averaged_stimuli_group2[0]), np.array(averaged_stimuli_group2[1])\n",
    "        \n",
    "print('Finished averaging stimuli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Virtual physiology\n",
    "\n",
    "# Constants\n",
    "OSI_THRESH = 0.4 # Both from Ko et al. (2011)\n",
    "DSI_THRESH = 0.3\n",
    "\n",
    "MEAN_RESPONSE_OFFSET = 5\n",
    "CURVE_FIT_OFFSET = 5\n",
    "\n",
    "SPATIAL_FREQUENCIES = np.arange(1/FRAME_SIZE, 0.5, 1/FRAME_SIZE) # Cycles / pixel\n",
    "ORIENTATIONS = np.arange(0, 360, 5) # Degrees\n",
    "TEMPORAL_FREQUENCIES = np.arange(0.05, 0.55, 0.05) # Cycles / time step\n",
    "\n",
    "def get_f_value (activity):\n",
    "    # Fit to sine\n",
    "    def func(x, a, b, c, d, e):\n",
    "        return e*x + a*np.sin(b*x + c) + d\n",
    "\n",
    "    params = []\n",
    "    mse = []\n",
    "    increment = []\n",
    "\n",
    "    MSE_threshold = 5\n",
    "\n",
    "    # Because the unit output has a floor of 0, the curve optimizer function might\n",
    "    # fail to fit the function unless we shift the unit's response up\n",
    "    # Shift it up by 0-150, returning the increment (shift), parameters and MSE for each increment\n",
    "    for i in range(0, 150, 5):\n",
    "        x = np.arange(len(activity[CURVE_FIT_OFFSET:]))\n",
    "        y = np.array(activity[CURVE_FIT_OFFSET:]) + i\n",
    "\n",
    "        # In case it fails to find optimal parameters, wrap in a try block\n",
    "        try:\n",
    "            bounds = (\n",
    "                [-np.inf, 0, -np.inf, -np.inf, -np.inf],\n",
    "                [np.inf, np.inf, np.inf, np.inf, np.inf] # Set lower bound of 1 for frequency\n",
    "            )\n",
    "            \n",
    "            optimizedParameters, pcov = opt.curve_fit(func, x, y, method='trf', bounds=bounds);\n",
    "            y_est = func(x, *optimizedParameters)\n",
    "            \n",
    "            params.append(optimizedParameters)\n",
    "            mse.append(np.mean(np.sum((y-y_est)**2)))\n",
    "            increment.append(i)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Return false if no good fits were found for any combination\n",
    "    if len(params):\n",
    "        # Get the index of the lowest MSE, use this to find the\n",
    "        # Corresponding parameters used\n",
    "        idx = np.argmin(mse)\n",
    "        best_params = params[idx]\n",
    "        best_i = increment[idx]\n",
    "        best_mse = mse[idx]\n",
    "\n",
    "        f0 = np.mean(activity[CURVE_FIT_OFFSET:]) # Average unit activity\n",
    "        f1 = (abs(best_params[0])) # Absolute of the amplitude of the fitted sine\n",
    "\n",
    "        y_est = func(np.arange(len(activity[CURVE_FIT_OFFSET:])), *best_params) - best_i\n",
    "    \n",
    "        if best_mse <= MSE_threshold and f0 != 0: # Reject f values for those units with poor sine fits\n",
    "            return f1/f0, y_est                   # as these are unlikely to be reliable\n",
    "        else:\n",
    "            return False, False\n",
    "    else:\n",
    "        return False, False\n",
    "\n",
    "def generate_gratings (spatial_freq, orient, temporal_freq, grating_amplitude=0.5, frames=30):\n",
    "    # Convert to measure relative to frame size\n",
    "    spatial_freq = spatial_freq*FRAME_SIZE\n",
    "    \n",
    "    gratings = []\n",
    "    for i in range(frames):\n",
    "        extended_val = 4 # For a full field grating, make it 4 times larger than\n",
    "                         # the frame so it can be cropped when rotated and still be full-field\n",
    "        extended_size = FRAME_SIZE*extended_val\n",
    "        crop_slice = slice( # Crop extended size grating to be the same size as the frame\n",
    "            (extended_size-FRAME_SIZE)//2, (extended_size-FRAME_SIZE)//2+FRAME_SIZE\n",
    "        )\n",
    "        \n",
    "        phase = 2*np.pi*i*temporal_freq # Shift  by a fraction of how many cycles per time step\n",
    "\n",
    "        x_range = np.linspace(0, 2*np.pi, num=extended_size)\n",
    "        y = np.sin(x_range*spatial_freq*extended_val + phase)*grating_amplitude\n",
    "\n",
    "        grating = np.repeat(y, extended_size, axis=0).reshape((extended_size, extended_size))\n",
    "        grating = ndimage.rotate(grating, -orient, reshape=False) # -orient to correct for 'default' counterclockwise rotation\n",
    "        grating = grating[crop_slice, crop_slice]\n",
    "        grating = grating.reshape(FRAME_SIZE**2)\n",
    "        gratings.append(grating)\n",
    "    \n",
    "    gratings = np.array([gratings])\n",
    "    gratings = torch.Tensor(gratings).to(DEVICE)\n",
    "\n",
    "    return gratings\n",
    "\n",
    "# Returns a dict of grating responses { unit_idx: responses_array }\n",
    "def get_grating_responses (model, grating, unit_idxs):\n",
    "    responses = {}\n",
    "    for idx in unit_idxs:\n",
    "        responses[idx] = []\n",
    "    \n",
    "    _, hidden_state = model(grating)\n",
    "    for t_step in range(WARMUP, WARMUP+T_STEPS):\n",
    "        for idx in unit_idxs:\n",
    "            unit_activity = hidden_state[0, t_step, idx]\n",
    "            responses[idx].append(unit_activity.cpu().detach().numpy())\n",
    "            \n",
    "    return responses\n",
    "\n",
    "# Takes orientation tuning curve at max tf and sf\n",
    "# Returns DSI, OSI\n",
    "def get_direction_orientation_selectivity (tuning_curve):    \n",
    "    dir_pref_idx = np.where(tuning_curve == np.max(tuning_curve))[0]\n",
    "    dir_pref = ORIENTATIONS[dir_pref_idx][0]\n",
    "    dir_pref_resp = tuning_curve[dir_pref_idx][0]\n",
    "    \n",
    "    dir_opp = (dir_pref + 180) % 360\n",
    "    dir_opp_idx = np.where(ORIENTATIONS == dir_opp)[0]\n",
    "    dir_opp_resp = tuning_curve[dir_opp_idx][0]\n",
    "    \n",
    "    orient_pref_resp = dir_pref_resp\n",
    "    \n",
    "    orient_orth = (dir_pref + 90) % 360\n",
    "    orient_orth_idx = np.where(ORIENTATIONS == orient_orth)[0]\n",
    "    orient_orth_resp = tuning_curve[orient_orth_idx][0]\n",
    "    \n",
    "    DSI = (dir_pref_resp - dir_opp_resp) / (dir_pref_resp + dir_opp_resp)\n",
    "    OSI = (orient_pref_resp - orient_orth_resp) / (orient_pref_resp + orient_orth_resp)\n",
    "    \n",
    "    return DSI, OSI\n",
    "\n",
    "# Returns array of physiology data\n",
    "# { \"unit_idx\", \"tuning_curve\", \"sf\", \"orientation\", \"preferred_response\", \"f\" }\n",
    "def get_physiology_data (averaged_hidden_units):\n",
    "    hidden_unit_rfs, hidden_unit_idxs = averaged_hidden_units    \n",
    "    \n",
    "    physiology_data = []\n",
    "\n",
    "    # Dictionary structures containing mean response and complete response for\n",
    "    # each sf, orient and tf combination\n",
    "    unit_responses = {}\n",
    "    mean_unit_responses = {}\n",
    "    for idx in hidden_unit_idxs:\n",
    "        unit_responses[idx] = np.zeros((\n",
    "            len(SPATIAL_FREQUENCIES),\n",
    "            len(ORIENTATIONS),\n",
    "            len(TEMPORAL_FREQUENCIES),\n",
    "            T_STEPS\n",
    "        ))\n",
    "        mean_unit_responses[idx] = np.zeros((\n",
    "            len(SPATIAL_FREQUENCIES),\n",
    "            len(ORIENTATIONS),\n",
    "            len(TEMPORAL_FREQUENCIES)\n",
    "        ))\n",
    "\n",
    "        \n",
    "    count = 0\n",
    "    max_count = mean_unit_responses[[k for k in mean_unit_responses.keys()][0]].size\n",
    "        \n",
    "    # Loop through each sf, orient, tf combination for each unit\n",
    "    # Add response and mean response to structures\n",
    "    for sf_idx, sf in enumerate(SPATIAL_FREQUENCIES):\n",
    "        for deg_idx, deg in enumerate(ORIENTATIONS):\n",
    "            for tf_idx, tf in enumerate(TEMPORAL_FREQUENCIES):\n",
    "                gratings = generate_gratings(sf, deg, tf, grating_amplitude=3, frames=WARMUP+T_STEPS)\n",
    "                grating_responses = get_grating_responses(model, gratings, hidden_unit_idxs)\n",
    "\n",
    "                for unit_idx in hidden_unit_idxs:\n",
    "                    unit_response = grating_responses[unit_idx]\n",
    "                    # Actual response\n",
    "                    unit_responses[unit_idx][sf_idx, deg_idx, tf_idx] = unit_response\n",
    "                    # Mean response\n",
    "                    mean_unit_responses[unit_idx][sf_idx, deg_idx, tf_idx] = np.mean(unit_response[MEAN_RESPONSE_OFFSET:])                                                                                        \n",
    "                \n",
    "                if count % 100 == 99:\n",
    "                    print(\"Finished param combination {}/{}\".format(count+1, max_count)) \n",
    "                count += 1\n",
    "    print(\"Finished tuning curve\")\n",
    "\n",
    "    for i, unit_idx in enumerate(mean_unit_responses):\n",
    "        # Get argmax of tuning curve parameters\n",
    "        tuning_curve = mean_unit_responses[unit_idx]\n",
    "        sf_idx, orient_idx, tf_idx = np.where(tuning_curve == np.max(tuning_curve))\n",
    "        \n",
    "        max_sf = SPATIAL_FREQUENCIES[sf_idx[0]]\n",
    "        max_orient = ORIENTATIONS[orient_idx[0]]\n",
    "        max_tf = TEMPORAL_FREQUENCIES[tf_idx[0]]\n",
    "        max_grating_response = unit_responses[unit_idx][sf_idx[0], orient_idx[0], tf_idx[0]]\n",
    "        \n",
    "        # Get f value and estimated sine curve fit\n",
    "        f, y_est = get_f_value(max_grating_response)\n",
    "        \n",
    "        # Get OSI and DSI measures\n",
    "        DSI, OSI = get_direction_orientation_selectivity(\n",
    "            tuning_curve[sf_idx[0], :, tf_idx[0]]\n",
    "        )\n",
    "        \n",
    "        # Append all physiology values to datastructure\n",
    "        physiology_data.append({\n",
    "            \"unit_idx\": unit_idx,\n",
    "            \"rf\": hidden_unit_rfs[np.where(hidden_unit_idxs == unit_idx)[0]],\n",
    "            \"tuning_curve\": tuning_curve,\n",
    "            \"sf\": max_sf,\n",
    "            \"orientation\": max_orient,\n",
    "            \"tf\": max_tf,\n",
    "            \"DSI\": DSI,\n",
    "            \"OSI\": OSI,\n",
    "            \"preferred_response\": max_grating_response,\n",
    "            \"mean_response\": np.mean(max_grating_response),\n",
    "            \"f\": f,\n",
    "            \"y_est\": y_est\n",
    "        })\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            print(\"Finished unit {} / {}\".format(i+1, len(hidden_unit_idxs)))\n",
    "\n",
    "    print(\"Finished all physiology data processing\")\n",
    "    \n",
    "    # Reject low mean response units (< 1% of max mean response)\n",
    "    physiology_data = np.array(physiology_data)\n",
    "    physiology_data_mean_resp = np.array([u[\"mean_response\"] for u in physiology_data])\n",
    "    thresh = max(physiology_data_mean_resp) * 0.01 # Get 1% of max mean value\n",
    "    physiology_data = physiology_data[physiology_data_mean_resp >= thresh]\n",
    "\n",
    "    return physiology_data\n",
    "\n",
    "\n",
    "\n",
    "# Spatial localization\n",
    "\n",
    "def generate_moving_bar (direction, x, y, bar_amplitude):\n",
    "    bar_size = 5\n",
    "\n",
    "    frames_len = 20\n",
    "    \n",
    "    stimuli = np.zeros((frames_len+10, FRAME_SIZE, FRAME_SIZE))\n",
    "\n",
    "    for i in range(frames_len):\n",
    "        bar_position = 0\n",
    "        square = np.ones((bar_size, bar_size))*bar_amplitude\n",
    "\n",
    "        if direction == 0 or direction == 270:\n",
    "            bar_position = (bar_size - i) % bar_size\n",
    "        else:\n",
    "            bar_position = i%(bar_size) \n",
    "\n",
    "        if direction == 0 or direction == 180:\n",
    "            square[bar_position, :] = -bar_amplitude\n",
    "        else:\n",
    "            square[:, bar_position] = -bar_amplitude\n",
    "\n",
    "        stimulus = np.zeros((FRAME_SIZE, FRAME_SIZE))\n",
    "        stimulus[y:y+bar_size, x:x+bar_size] = square\n",
    "\n",
    "        stimuli[i+10, :, :] = stimulus\n",
    "\n",
    "    stimuli = stimuli.reshape(frames_len+10, FRAME_SIZE**2)\n",
    "    stimuli = torch.Tensor(stimuli).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    return stimuli\n",
    "\n",
    "def get_RF_responses (model, stimuli, unit_idxs):\n",
    "    responses = {}\n",
    "    for idx in unit_idxs:\n",
    "        responses[idx] = []\n",
    "    \n",
    "    _, hidden_state = model(stimuli)\n",
    "    for t_step in range(stimuli.shape[1]):\n",
    "        for idx in unit_idxs:\n",
    "            unit_activity = hidden_state[0, t_step, idx]\n",
    "            responses[idx].append(unit_activity.cpu().detach().numpy())\n",
    "            \n",
    "    return responses\n",
    "\n",
    "def get_unit_heatmap (unit, bar_amplitude):\n",
    "    unit_idx = unit[\"unit_idx\"]\n",
    "    responses = np.zeros((4, 15, 15))\n",
    "\n",
    "    for orient_idx, orient in enumerate([0, 90, 180, 270]):\n",
    "        for row_idx, row in enumerate(np.arange(15)):\n",
    "            for col_idx, col in enumerate(np.arange(15)):\n",
    "                stimuli = generate_moving_bar(orient, col, row, bar_amplitude)\n",
    "                resp = get_RF_responses(model, stimuli, [unit_idx])[unit_idx]\n",
    "                resp = resp[10:]-np.mean(resp[:10])\n",
    "                responses[orient_idx, row_idx, col_idx] = np.mean(resp)\n",
    "                \n",
    "    unit[\"rf_heatmap\"] = np.mean(responses, 0)\n",
    "\n",
    "def process_unit_heatmaps (units):\n",
    "    for i, unit in enumerate(units):\n",
    "        get_unit_heatmap(unit, 0.5)\n",
    "        print(\"Processed {} / {}\".format(i+1, len(units)))\n",
    "    print('\\nFinished\\n')\n",
    "    \n",
    "def get_unit_heatmap_centre (unit):\n",
    "    response = unit[\"rf_heatmap\"] \n",
    "    smoothed = ndimage.gaussian_filter(response, 1)\n",
    "\n",
    "    thresh = np.max(smoothed) - np.std(smoothed)\n",
    "    thresholded = smoothed > thresh\n",
    "\n",
    "    centre = ndimage.center_of_mass(thresholded)\n",
    "    scale_factor = FRAME_SIZE/response.shape[0]\n",
    "    \n",
    "    unit[\"position\"] = (centre[0] * scale_factor, centre[1] * scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run virtual physiology code on model units\n",
    "\n",
    "physiology_data_group1 = get_physiology_data(averaged_stimuli_group1)\n",
    "physiology_data_group2 = get_physiology_data(averaged_stimuli_group2)\n",
    "    \n",
    "process_unit_heatmaps(physiology_data_group1)\n",
    "process_unit_heatmaps(physiology_data_group2)\n",
    "\n",
    "for unit in physiology_data_group1:\n",
    "    get_unit_heatmap_centre (unit)\n",
    "print(\"Done 1\")\n",
    "    \n",
    "for unit in physiology_data_group2:\n",
    "    get_unit_heatmap_centre (unit)\n",
    "print(\"Done 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot spike triggered averages and input weights\n",
    "def plot_weights (weights_array, rows, columns, offset = 0, title = \"\"):\n",
    "    font_size = 5\n",
    "    weights = weights_array[offset:offset+rows*columns, :]\n",
    "    im = np.zeros((rows*FRAME_SIZE, columns*FRAME_SIZE))\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for column in range(columns):\n",
    "            idx = row*columns + column\n",
    "            hidden_unit_weights = np.zeros(FRAME_SIZE**2) # Pad image if no more weights to show\n",
    "            if idx < len(weights):\n",
    "                hidden_unit_weights = weights[idx]\n",
    "            hidden_unit_weights = np.reshape(hidden_unit_weights, (FRAME_SIZE, FRAME_SIZE))\n",
    "            im[row*FRAME_SIZE:(row+1)*FRAME_SIZE, column*FRAME_SIZE:(column+1)*FRAME_SIZE] = hidden_unit_weights\n",
    "\n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    im_plt = plt.imshow(im, extent=[0, columns, 0, rows], cmap='gray')\n",
    "    \n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    cb_plt = plt.colorbar(im_plt, cax=cax)\n",
    "    cb_plt.ax.tick_params(labelsize=font_size)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    #ax.set_xticklabels(np.arange(0, columns+1), size=font_size)\n",
    "    ax.set_yticks([])\n",
    "    #ax.set_yticklabels(np.arange(0, rows+1), size=font_size)\n",
    "    #ax.grid(which='major', color='w', li.2estyle='-', linewidth=0.5)\n",
    "    ax.set_title(title, size=font_size)\n",
    "\n",
    "rows, columns, offset = (28, 28, 0)\n",
    "plot_weights(model.rnn.weight_ih_l0.cpu().detach().numpy(), rows, columns, offset, \"Input weights\")\n",
    "\n",
    "plot_weights(normalize(averaged_stimuli_group1[0]), rows, columns, offset, \"Normalized group 1 STAs (H-H weights)\")\n",
    "plot_weights(max_abs_normalize(averaged_stimuli_group1[0]), rows, columns, offset, \"MaxAbs normalized group 1 STAs (H-H weights)\")\n",
    "plot_weights(normalize(averaged_stimuli_group2[0]), rows, columns, offset, \"Normalized Group 2 STAs (H-H weights)\")\n",
    "plot_weights(max_abs_normalize(averaged_stimuli_group2[0]), rows, columns, offset, \"MaxAbs normalized group 2 STAs (H-H weights)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grating_animation (gratings): # Will only work when called (or output called)\n",
    "                                       # at end of notebook\n",
    "    fig = plt.figure(dpi=40)\n",
    "    init_data = gratings.cpu().detach().numpy()[0, 0].reshape(FRAME_SIZE, FRAME_SIZE)\n",
    "    im = plt.imshow(init_data, cmap='gray')\n",
    "    plt.title(\"Grating stimulus\", size=20)\n",
    "    plt.close()\n",
    "    def animate(i):\n",
    "        im_data = gratings.cpu().detach().numpy()[0, i].reshape(FRAME_SIZE, FRAME_SIZE)\n",
    "        im.set_array(im_data)\n",
    "        return [im]\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate,\n",
    "        frames=gratings.shape[1], interval=100\n",
    "    )\n",
    "    \n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "# Graphs is an array of [\"rf\", \"tuning_curve\", \"preferred_response\"]\n",
    "def plot_physiology_data (unit, graphs):\n",
    "    if \"rf\" in graphs:\n",
    "        sta = normalize(unit[\"rf\"])\n",
    "        fig = plt.figure(dpi=40)\n",
    "        plt.imshow(sta.reshape(FRAME_SIZE, FRAME_SIZE), cmap='gray')\n",
    "        plt.title('Normalized SA', size=25)\n",
    "        plt.show()\n",
    "        \n",
    "    if \"fitted_gabor\" in graphs:\n",
    "        gratings = generate_gabors(\n",
    "            unit[\"sf\"],\n",
    "            unit[\"orientation\"],\n",
    "            unit[\"tf\"],\n",
    "            5,\n",
    "            unit[\"position\"][0],\n",
    "            unit[\"position\"][1]\n",
    "        )\n",
    "        gratings = gratings.cpu().detach().numpy()\n",
    "        fig = plt.figure(dpi=40)\n",
    "        plt.imshow(gratings[0, 0, :].reshape(FRAME_SIZE, FRAME_SIZE), cmap='gray', vmin=-1, vmax=1)\n",
    "        plt.title(\"Fitted Gabor\", size=25)\n",
    "\n",
    "    if \"tuning_curve\" in graphs:\n",
    "        tf_idx = np.where(TEMPORAL_FREQUENCIES == unit[\"tf\"])[0][0]\n",
    "\n",
    "        fig, ax = plt.subplots(dpi=100)\n",
    "        plt.imshow(unit[\"tuning_curve\"][:, :, tf_idx])\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(\"Orientation\")\n",
    "        plt.xticks(range(len(ORIENTATIONS)), ORIENTATIONS, size=6)\n",
    "        plt.ylabel(\"SF\")\n",
    "        plt.yticks(range(len(SPATIAL_FREQUENCIES)), np.round(SPATIAL_FREQUENCIES, 2), size=6)\n",
    "        plt.title(\"Mean unit activity as a function of orientation and SF\", size=8)\n",
    "        \n",
    "        for index, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "            if (index % 4) != 0:\n",
    "                label.set_visible(False)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    if \"preferred_response\" in graphs:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(unit[\"preferred_response\"])\n",
    "        if unit[\"f\"]:\n",
    "            plt.plot(np.arange(CURVE_FIT_OFFSET, T_STEPS), unit[\"y_est\"])\n",
    "        plt.xlabel(\"Frame\")\n",
    "        plt.ylabel(\"Activity\")\n",
    "        plt.title(\n",
    "            \"F = {}, SF = {}, orientation = {}, TF = {}, unit {}\".format(\n",
    "                round(unit[\"f\"],2), round(unit[\"sf\"], 2), unit[\"orientation\"], round(unit[\"tf\"],2), unit[\"unit_idx\"]\n",
    "            )\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "    if \"polar_tuning_curve\" in graphs:\n",
    "        tf_idx = np.where(TEMPORAL_FREQUENCIES == unit[\"tf\"])[0][0]\n",
    "        \n",
    "        fig = plt.figure(dpi=75)\n",
    "        \n",
    "        rad = SPATIAL_FREQUENCIES\n",
    "        azm = np.radians(ORIENTATIONS)\n",
    "        r, th = np.meshgrid(rad, azm)\n",
    "        z = unit[\"tuning_curve\"][:, :, tf_idx].transpose()\n",
    "\n",
    "        ax = plt.subplot(projection=\"polar\")\n",
    "        im = plt.pcolormesh(th, r, z, shading='gouraud') # shading='auto', 'nearest' or 'gouraud'\n",
    "        plt.grid()\n",
    "\n",
    "        plt.setp(ax.get_yticklabels(), color=\"w\")\n",
    "        ax.set_rticks(np.round(rad, 2))\n",
    "        ax.set_rlabel_position(0)\n",
    "        ax.set_ylim(0,max(rad))\n",
    "        ax.set_theta_direction(-1)\n",
    "        ax.set_theta_zero_location('N')\n",
    "        plt.title(\"Response as a function of orientation and SF\", size=10)\n",
    "        \n",
    "        cbar = plt.colorbar(im, pad=0.1)\n",
    "        cbar.set_label('Mean response')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "for i, unit in enumerate(physiology_data_group2[:300]):\n",
    "    plot_physiology_data (unit, [\"preferred_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spatial responses\n",
    "\n",
    "fig = plt.figure(dpi=300, figsize=(6,4))\n",
    "\n",
    "for i, unit in enumerate(physiology_data_group2[:120]):\n",
    "    ax = fig.add_subplot(6, 20, i+1, xticks=[], yticks=[])\n",
    "    response = unit[\"rf_heatmap\"] \n",
    "    ax.imshow(response)\n",
    "    \n",
    "    centre = unit[\"position\"]\n",
    "    if not np.isnan(centre[0]):\n",
    "        ax.scatter([ int(centre[1]*0.75) ], [ int(centre[0]*0.75) ], color='red', s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise weights between recurrent units\n",
    "\n",
    "starting_idx = 25\n",
    "n_units = 10\n",
    "physiology_data = physiology_data_group2[starting_idx:starting_idx+n_units]\n",
    "\n",
    "hh_weights = model.rnn.weight_hh_l0.cpu().detach().numpy()\n",
    "hh_weights_subset = np.zeros(shape=(n_units, n_units))\n",
    "\n",
    "for col, pre_unit in enumerate(physiology_data):\n",
    "    for row, post_unit in enumerate(physiology_data):\n",
    "        hh_weights_subset[row, col] = hh_weights[post_unit[\"unit_idx\"], pre_unit[\"unit_idx\"]]\n",
    "\n",
    "RFs = normalize([u[\"rf\"] for u in physiology_data])\n",
    "\n",
    "cols = n_units + 1\n",
    "rows = n_units + 1\n",
    "\n",
    "fig, f_axs = plt.subplots(\n",
    "    dpi=35,\n",
    "    ncols=cols,\n",
    "    nrows=rows,\n",
    "    figsize=(cols, rows),\n",
    "    gridspec_kw=dict(wspace=0.0, hspace=0.0,\n",
    "                     top=1. - 0.5 / (rows + 1), bottom=0.5 / (rows + 1),\n",
    "                     left=0.5 / (cols + 1), right=1 - 0.5 / (cols + 1)),\n",
    ")\n",
    "gs = f_axs[0, 0].get_gridspec()\n",
    "\n",
    "for axs in f_axs[1:, 1:]:\n",
    "    for ax in axs:\n",
    "        ax.remove()\n",
    "axbig = fig.add_subplot(gs[1:, 1:])\n",
    "axbig.set_axis_off()\n",
    "im = axbig.imshow(hh_weights_subset, cmap='bwr', vmax=0.5, vmin=-0.5)\n",
    "\n",
    "cbaxes = fig.add_axes([1, 0.04, 0.03, 0.915]) \n",
    "cb = plt.colorbar(im, cax = cbaxes)\n",
    "for t in cb.ax.get_yticklabels():\n",
    "    t.set_fontsize(20)\n",
    "\n",
    "count = 0\n",
    "for ax_row in f_axs:\n",
    "    for ax in ax_row:\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        if count > 0 and count < n_units+1: # X-axis row\n",
    "            unit_n = count-1\n",
    "        elif count % (n_units+1) == 0: # Y-axis row\n",
    "            unit_n = count//(n_units+1) - 1\n",
    "        else:\n",
    "            unit_n = -1\n",
    "            \n",
    "        if unit_n > -1:\n",
    "            unit = np.reshape(RFs[unit_n], (FRAME_SIZE, FRAME_SIZE))\n",
    "            ax.imshow(unit, cmap='bwr')\n",
    "            \n",
    "        count += 1\n",
    "        \n",
    "fig.text(0.55, 1, 'Presynaptic unit', ha='center', fontsize=35)\n",
    "fig.text(-0.025, 0.5, 'Postsynaptic unit', va='center', rotation='vertical', fontsize=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of orientations/direction selectivity indices\n",
    "\n",
    "# Get selectivity measures\n",
    "OSIs_group1 = [u[\"OSI\"] for u in physiology_data_group1]\n",
    "OSIs_group2 = [u[\"OSI\"] for u in physiology_data_group2]\n",
    "\n",
    "DSIs_group1 = [u[\"DSI\"] for u in physiology_data_group1]\n",
    "DSIs_group2 = [u[\"DSI\"] for u in physiology_data_group2]\n",
    "\n",
    "# Calculate percent orientation selective\n",
    "percent_OSI_selective_group1 = round(len([osi for osi in OSIs_group1 if osi > OSI_THRESH]) / len(physiology_data_group1) * 100)\n",
    "percent_OSI_selective_group2 = round(len([osi for osi in OSIs_group2 if osi > OSI_THRESH]) / len(physiology_data_group2) * 100)\n",
    "\n",
    "# Calculate percent direction selective\n",
    "percent_DSI_selective_group1 = round(len([dsi for dsi in DSIs_group1 if dsi > DSI_THRESH]) / len(physiology_data_group1) * 100)\n",
    "percent_DSI_selective_group2 = round(len([dsi for dsi in DSIs_group2 if dsi > DSI_THRESH]) / len(physiology_data_group2) * 100)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=plt.figaspect(1/2))\n",
    "\n",
    "bins = 10\n",
    "\n",
    "axs[0, 0].hist(OSIs_group1, bins=bins, color=GROUP1_COLOR)\n",
    "axs[0, 0].set_title(\"Group 1 ({}% orient. selective)\".format(percent_OSI_selective_group1))\n",
    "axs[0, 0].set_xlabel(\"OSI value\")\n",
    "axs[0, 1].hist(OSIs_group2, bins=bins, color=GROUP2_COLOR)\n",
    "axs[0, 1].set_title(\"Group 2 ({}% orient. selective)\".format(percent_OSI_selective_group2))\n",
    "axs[0, 1].set_xlabel(\"OSI value\")\n",
    "\n",
    "axs[1, 0].hist(DSIs_group1, bins=bins, color=GROUP1_COLOR)\n",
    "axs[1, 0].set_title(\"Group 1 ({}% dir. selective)\".format(percent_DSI_selective_group1))\n",
    "axs[1, 0].set_xlabel(\"DSI value\")\n",
    "axs[1, 1].hist(DSIs_group2, bins=bins, color=GROUP2_COLOR)\n",
    "axs[1, 1].set_title(\"Group 2 ({}% dir. selective)\".format(percent_DSI_selective_group2))\n",
    "axs[1, 1].set_xlabel(\"DSI value\")\n",
    "\n",
    "plt.suptitle(\"Distribution of OSI and DSI values\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot other data distributions\n",
    "\n",
    "distributions = [\n",
    "    { \"field_name\": \"temporal frequency\", \"x_axis\": \"Cycles / second\", \"field\": \"tf\", \"scale\": 25 },\n",
    "    { \"field_name\": \"spatial frequency\", \"x_axis\": \"Cycles / pixel\", \"field\": \"sf\", \"scale\": 1 }\n",
    "]\n",
    "\n",
    "\n",
    "for distribution in distributions:\n",
    "    field = distribution[\"field\"]\n",
    "    field_name = distribution[\"field_name\"]\n",
    "    scale = distribution[\"scale\"]\n",
    "    x_axis = distribution[\"x_axis\"]\n",
    "    \n",
    "    # Get selectivity measures\n",
    "    values_group1 = np.array([u[field] for u in physiology_data_group1]) * scale\n",
    "    values_group2 = np.array([u[field] for u in physiology_data_group2]) * scale\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=plt.figaspect(1/2))\n",
    "\n",
    "    bins = 8\n",
    "\n",
    "    axs[0].hist(values_group1, bins=bins, color=GROUP1_COLOR)\n",
    "    axs[0].set_title(\"Group 1\")\n",
    "    axs[0].set_xlabel(x_axis)\n",
    "\n",
    "    axs[1].hist(values_group2, bins=bins, color=GROUP2_COLOR)\n",
    "    axs[1].set_title(\"Group 2\")\n",
    "    axs[1].set_xlabel(x_axis)\n",
    "\n",
    "    plt.suptitle(\"Distribution of {} values\".format(field_name))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_tailed_p_value (null_distribution, test_statistic):\n",
    "    null = np.array(null_distribution)\n",
    "    p_left = len(np.where(null <= test_statistic)[0]) / len(null)\n",
    "    p_right = len(np.where(null > test_statistic)[0]) / len(null)\n",
    "\n",
    "    p =  2 * min(p_left, p_right)\n",
    "    \n",
    "    return p, p_left, p_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 5b\n",
    "\n",
    "xlabels = [0, 45, 90, 135]\n",
    "xpos = [0, 1, 2, 3]\n",
    "\n",
    "def get_orientation_counts (physiology_data):\n",
    "    orientations = [0, 0, 0, 0]\n",
    "    \n",
    "    for unit in physiology_data:\n",
    "        if unit[\"OSI\"] > OSI_THRESH:\n",
    "            direction = unit[\"orientation\"]\n",
    "            if direction > 337.5 or direction <= 22.5:\n",
    "                orientations[0] += 1\n",
    "            elif direction > 22.5 and direction <= 67.5:\n",
    "                orientations[1] += 1\n",
    "            elif direction > 67.5 and direction <= 112.5:\n",
    "                orientations[2] += 1\n",
    "            elif direction > 112.5 and direction <= 157.5:\n",
    "                orientations[3] += 1\n",
    "            elif direction > 157.5 and direction <= 202.5:\n",
    "                orientations[0] += 1\n",
    "            elif direction > 202.5 and direction <= 247.5:\n",
    "                orientations[1] += 1\n",
    "            elif direction > 247.5 and direction <= 297.5:\n",
    "                orientations[2] += 1\n",
    "            else:\n",
    "                orientations[3] += 1\n",
    "    return orientations\n",
    "\n",
    "# Array of tuples (5% ci, 95% ci) for each orientation bin\n",
    "def get_null_orientation_distribution (physiology_data, xpos, iteration_len):\n",
    "    orientation_len = len(xpos)\n",
    "    orientations = np.zeros((iteration_len, orientation_len))\n",
    "    \n",
    "    selectivity_key = \"OSI\" if orientation_len == 4 else \"DSI\"\n",
    "    selectivity_thresh = OSI_THRESH if orientation_len == 4 else DSI_THRESH\n",
    "    \n",
    "    for i in range(iteration_len):\n",
    "        for unit in physiology_data:\n",
    "            if unit[selectivity_key] > selectivity_thresh:\n",
    "                orientations[i, np.random.randint(low=0, high=orientation_len)] += 1\n",
    "    \n",
    "    upper = np.percentile(orientations, 97.5, axis=0)\n",
    "    lower = np.percentile(orientations, 2.5, axis=0)\n",
    "        \n",
    "    ci = [(lower[i], upper[i]) for i in range(orientation_len)]\n",
    "        \n",
    "    return orientations, ci\n",
    "\n",
    "def plot_cis (ax, cis):\n",
    "    for i, ci in enumerate(cis):\n",
    "        height = ci[1]-ci[0]\n",
    "        ax.add_patch(Rectangle(\n",
    "            (-0.4 + i, ci[0]),\n",
    "            0.8, height,\n",
    "            facecolor=NEUTRAL_COLOR, alpha=0.75\n",
    "        ))\n",
    "\n",
    "group1_orientations = get_orientation_counts(physiology_data_group1)\n",
    "group2_orientations = get_orientation_counts(physiology_data_group2)\n",
    "\n",
    "group1_null_distribution, group1_null_ci = get_null_orientation_distribution(physiology_data_group1, xpos, 10000)\n",
    "group2_null_distribution, group2_null_ci = get_null_orientation_distribution(physiology_data_group2, xpos, 10000)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=plt.figaspect(1/2))\n",
    "\n",
    "axs[0].bar(xpos, group1_orientations, color=GROUP1_COLOR)\n",
    "axs[0].set_xticks(xpos)\n",
    "axs[0].set_xticklabels(xlabels)\n",
    "axs[0].set_xlabel(\"Preferred orientation (°)\")\n",
    "axs[0].set_ylabel(\"Number of units\")\n",
    "axs[0].set_title(\"Group 1\")\n",
    "plot_cis(axs[0], group1_null_ci)\n",
    "\n",
    "axs[1].bar(xpos, group2_orientations, color=GROUP2_COLOR)\n",
    "axs[1].set_xticks(xpos)\n",
    "axs[1].set_xticklabels(xlabels)\n",
    "axs[1].set_xlabel(\"Preferred orientation (°)\")\n",
    "axs[1].set_title(\"Group 2\")\n",
    "plot_cis(axs[1], group2_null_ci)\n",
    "\n",
    "plt.suptitle('Distributions of preferred orientations')\n",
    "plt.show()\n",
    "\n",
    "group1_pvalues = []\n",
    "group2_pvalues = []\n",
    "for i in range(4):\n",
    "    p1, _, _ = get_two_tailed_p_value(group1_null_distribution[:, i], group1_orientations[i])\n",
    "    p2, _, _ = get_two_tailed_p_value(group2_null_distribution[:, i], group2_orientations[i])\n",
    "    group1_pvalues.append(p1)\n",
    "    group2_pvalues.append(p2)\n",
    "\n",
    "print(group1_pvalues)\n",
    "print(group2_pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 6b\n",
    "\n",
    "xlabels = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "xpos = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "def get_direction_counts (physiology_data):\n",
    "    orientations = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    for unit in physiology_data:\n",
    "        if unit[\"DSI\"] > DSI_THRESH:\n",
    "            direction = unit[\"orientation\"]\n",
    "            if direction > 337.5 or direction <= 22.5:\n",
    "                orientations[0] += 1\n",
    "            elif direction > 22.5 and direction <= 67.5:\n",
    "                orientations[1] += 1\n",
    "            elif direction > 67.5 and direction <= 112.5:\n",
    "                orientations[2] += 1\n",
    "            elif direction > 112.5 and direction <= 157.5:\n",
    "                orientations[3] += 1\n",
    "            elif direction > 157.5 and direction <= 202.5:\n",
    "                orientations[4] += 1\n",
    "            elif direction > 202.5 and direction <= 247.5:\n",
    "                orientations[5] += 1\n",
    "            elif direction > 247.5 and direction <= 297.5:\n",
    "                orientations[6] += 1\n",
    "            else:\n",
    "                orientations[7] += 1\n",
    "    return orientations\n",
    "\n",
    "group1_directions = get_direction_counts(physiology_data_group1)\n",
    "group2_directions = get_direction_counts(physiology_data_group2)\n",
    "\n",
    "group1_null_distribution, group1_null_ci = get_null_orientation_distribution(physiology_data_group1, xpos, 10000)\n",
    "group2_null_distribution, group2_null_ci = get_null_orientation_distribution(physiology_data_group2, xpos, 10000)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=plt.figaspect(1/3))\n",
    "\n",
    "axs[0].bar(xpos, group1_directions, color=GROUP1_COLOR)\n",
    "axs[0].set_xticks(xpos)\n",
    "axs[0].set_xticklabels(xlabels)\n",
    "axs[0].set_xlabel(\"Preferred direction (°)\")\n",
    "axs[0].set_ylabel(\"Number of units\")\n",
    "axs[0].set_title(\"Group 1\")\n",
    "plot_cis(axs[0], group1_null_ci)\n",
    "\n",
    "axs[1].bar(xpos, group2_directions, color=GROUP2_COLOR)\n",
    "axs[1].set_xticks(xpos)\n",
    "axs[1].set_xticklabels(xlabels)\n",
    "axs[1].set_xlabel(\"Preferred direction (°)\")\n",
    "axs[1].set_title(\"Group 2\")\n",
    "plot_cis(axs[1], group2_null_ci)\n",
    "\n",
    "plt.suptitle('Distributions of preferred directions')\n",
    "plt.show()\n",
    "\n",
    "group1_pvalues = []\n",
    "group2_pvalues = []\n",
    "for i in range(8):\n",
    "    p1, _, _ = get_two_tailed_p_value(group1_null_distribution[:, i], group1_directions[i])\n",
    "    p2, _, _ = get_two_tailed_p_value(group2_null_distribution[:, i], group2_directions[i])\n",
    "    group1_pvalues.append(p1)\n",
    "    group2_pvalues.append(p2)\n",
    "\n",
    "print(group1_pvalues)\n",
    "print(group2_pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marques plots\n",
    "\n",
    "# Returns relative x and y with respect to V2 preferred orientation\n",
    "def get_dx_dy (V1_unit, V2_unit):\n",
    "    rad_angle = V2_unit[\"orientation\"] * math.pi/180 # Orientation angle in radians\n",
    "    rotation_matrix = np.array([\n",
    "        [math.cos(-rad_angle), -math.sin(-rad_angle)],\n",
    "        [math.sin(-rad_angle),  math.cos(-rad_angle)]\n",
    "    ])\n",
    "    \n",
    "    V1_point = np.array([V1_unit[\"position\"][1], V1_unit[\"position\"][0]]) # Reverse because stored as row, col\n",
    "    V2_point = np.array([V2_unit[\"position\"][1], V2_unit[\"position\"][0]])\n",
    "    \n",
    "    V1_point_rot = np.dot(rotation_matrix, V1_point)\n",
    "    V2_point_rot = np.dot(rotation_matrix, V2_point)\n",
    "    \n",
    "    dx = V1_point_rot[0] - V2_point_rot[0]\n",
    "    dy = V1_point_rot[1] - V2_point_rot[1]\n",
    "    \n",
    "    return dx, dy\n",
    "\n",
    "# Bearing with respect to 0, 0 point\n",
    "def get_bearing (unit_pos):\n",
    "    rad2deg = 57.2957795130823209\n",
    "    centre_pos = (0, 0)\n",
    "\n",
    "    if centre_pos[0] == unit_pos[0] and centre_pos[1] == unit_pos[1]:\n",
    "        return -1\n",
    "    else:\n",
    "        theta = math.atan2(unit_pos[0] - centre_pos[0], unit_pos[1] - centre_pos[1]);\n",
    "        if (theta < 0.0):\n",
    "            theta += math.pi*2\n",
    "        return rad2deg*theta\n",
    "    \n",
    "# Returns units split into 8 angular bins based on their RF centres\n",
    "def get_angular_bins (x, y):\n",
    "    orientations = {\n",
    "        0: [],\n",
    "        45: [],\n",
    "        90: [],\n",
    "        135: [],\n",
    "        180: [],\n",
    "        225: [],\n",
    "        270: [],\n",
    "        315: []\n",
    "    }\n",
    "    \n",
    "    points = [(x[i], y[i]) for i in range(len(x))]\n",
    "    \n",
    "    for point in points:\n",
    "        if point[0] == -1:\n",
    "            continue\n",
    "            \n",
    "        bearing = get_bearing(point)\n",
    "\n",
    "        if bearing > 337.5 or bearing <= 22.5:\n",
    "            orientations[0].append(point)\n",
    "        elif bearing > 22.5 and bearing <= 67.5:\n",
    "            orientations[45].append(point)\n",
    "        elif bearing > 67.5 and bearing <= 112.5:\n",
    "            orientations[90].append(point)\n",
    "        elif bearing > 112.5 and bearing <= 157.5:\n",
    "            orientations[135].append(point)\n",
    "        elif bearing > 157.5 and bearing <= 202.5:\n",
    "            orientations[180].append(point)\n",
    "        elif bearing > 202.5 and bearing <= 247.5:\n",
    "            orientations[225].append(point)\n",
    "        elif bearing > 247.5 and bearing <= 297.5:\n",
    "            orientations[270].append(point)\n",
    "        else:\n",
    "            orientations[315].append(post_unit)\n",
    "\n",
    "    return orientations\n",
    "\n",
    "# Array of tuples (5% ci, 95% ci) for each orientation bin\n",
    "def get_null_angular_distribution (total_points, iterations, model_angular_bins):\n",
    "    angular_bins_len = 4\n",
    "    angular_bins = np.zeros((iterations, angular_bins_len))\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        for unit in range(total_points):\n",
    "            orientation_idx = np.random.randint(low=0, high=angular_bins_len)\n",
    "            angular_bins[i, orientation_idx] += 1\n",
    "        if i % 500 == 0:\n",
    "            print('Iteration', i)\n",
    "                \n",
    "    angular_bins = (angular_bins-np.mean(model_angular_bins))/np.std(model_angular_bins)\n",
    "                \n",
    "    upper = np.percentile(angular_bins, 97.5, axis=0)\n",
    "    lower = np.percentile(angular_bins, 2.5, axis=0)\n",
    "        \n",
    "    cis = [(lower[i], upper[i]) for i in range(angular_bins_len)]\n",
    "        \n",
    "    return angular_bins, cis\n",
    "\n",
    "def plot_angular_cis (ax, cis):\n",
    "    for i, ci in enumerate(cis):\n",
    "        height = ci[1]-ci[0]\n",
    "        x = i * 0.75 + 0.25*(i-1)\n",
    "        \n",
    "        ax.add_patch(Rectangle(\n",
    "            (x, ci[0]),\n",
    "            0.5, height,\n",
    "            facecolor=NEUTRAL_COLOR, alpha=0.75\n",
    "        ))\n",
    "\n",
    "weights = model.rnn.weight_hh_l0.cpu().detach().numpy()\n",
    "normed_weights = (weights-np.mean(weights))/np.std(weights)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "weights = []\n",
    "\n",
    "for pre_unit in physiology_data_group2:\n",
    "    for post_unit in physiology_data_group1:\n",
    "        connection_weight = normed_weights[post_unit[\"unit_idx\"], pre_unit[\"unit_idx\"]]\n",
    "        \n",
    "        if (\n",
    "            pre_unit[\"position\"][0] != -1 and post_unit[\"position\"][0] != -1 and\n",
    "            pre_unit[\"OSI\"] > OSI_THRESH   and post_unit[\"OSI\"] > OSI_THRESH  and\n",
    "            abs(connection_weight) > 2.5\n",
    "        ):\n",
    "            dx, dy = get_dx_dy(V1_unit=post_unit, V2_unit=pre_unit)\n",
    "            x.append(dx)\n",
    "            y.append(dy)\n",
    "            weights.append(abs(connection_weight))\n",
    "    \n",
    "print('x mean = {}, std = {}\\ny mean = {}, std = {}'.format(\n",
    "    np.nanmean(x),\n",
    "    np.nanstd(x),\n",
    "    np.nanmean(y),\n",
    "    np.nanstd(y)\n",
    "))\n",
    "print('\\nDifference = {} (> 1 indicates greater spread along y)'.format(np.nanstd(y)-np.nanstd(x)))\n",
    "\n",
    "hist, x_edge, y_edge = np.histogram2d(x, y, [np.arange(-20, 21), np.arange(-20, 21)])\n",
    "extent = [x_edge[0], x_edge[-1], y_edge[0], y_edge[-1]]\n",
    "plt.imshow(hist.T, extent=extent, origin='lower')\n",
    "plt.xlabel(\"Δ x\")\n",
    "plt.ylabel(\"Δ y\")\n",
    "plt.title(\"Change in relative RF coordiantes\")\n",
    "plt.show()\n",
    "\n",
    "angular_bins = get_angular_bins(x, y)\n",
    "angular_bin_counts = [\n",
    "    len(angular_bins[0]) + len(angular_bins[180]),\n",
    "    len(angular_bins[45]) + len(angular_bins[225]),\n",
    "    len(angular_bins[90]) + len(angular_bins[270]),\n",
    "    len(angular_bins[135]) + len(angular_bins[315])\n",
    "]\n",
    "#angular_bin_counts = [len(angular_bins[key]) for key in angular_bins.keys()]\n",
    "null_angular_distribution, angular_bin_cis = get_null_angular_distribution(\n",
    "    total_points=len(x),\n",
    "    iterations=10000,\n",
    "    model_angular_bins=angular_bin_counts\n",
    ")\n",
    "normed_angular_bin_counts = (angular_bin_counts - np.mean(angular_bin_counts))/np.std(angular_bin_counts)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(normed_angular_bin_counts, '.-')\n",
    "plot_angular_cis(ax, angular_bin_cis)\n",
    "ax.set_ylabel(\"Normalized count\")\n",
    "ax.set_xlabel(\"Angular bin\")\n",
    "ax.set_xticks([0, 1, 2, 3])#, 4, 5, 6, 7])\n",
    "ax.set_xticklabels([\"0\", \"45\", \"90\", \"135\"])# \"180\", \"225\", \"270\", \"315\"])\n",
    "plt.show()\n",
    "\n",
    "pvalues = []\n",
    "for i in range(4):\n",
    "    p, _, _ = get_two_tailed_p_value(null_angular_distribution[:, i], normed_angular_bin_counts[i])\n",
    "    pvalues.append(p)\n",
    "\n",
    "print(pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight strength as a function of orientation difference\n",
    "\n",
    "weights = model.rnn.weight_hh_l0.cpu().detach().numpy()\n",
    "\n",
    "def get_difference_values (post_units, pre_units, mode):\n",
    "    difference_values = []\n",
    "    weight_strengths = []\n",
    "    \n",
    "    max_value, mod_value = (90, 180) if mode == 'orientation' else (180, 360)\n",
    "    selectivity_index = 'OSI' if mode == 'orientation' else 'DSI'\n",
    "    selectivity_thresh = OSI_THRESH if mode == 'orientation' else DSI_THRESH\n",
    "\n",
    "    for post_unit in post_units:\n",
    "        for pre_unit in pre_units:\n",
    "            connection_weight = weights[post_unit[\"unit_idx\"], pre_unit[\"unit_idx\"]]\n",
    "            a1, a2 = pre_unit[\"orientation\"], post_unit[\"orientation\"]\n",
    "            a_delta = mod_value - abs(abs((a1 - a2)%max_value) - mod_value);\n",
    "\n",
    "            if (\n",
    "                post_unit[selectivity_index] > selectivity_thresh and\n",
    "                pre_unit[selectivity_index] > selectivity_thresh\n",
    "            ):\n",
    "                difference_values.append(a_delta)\n",
    "                weight_strengths.append(connection_weight)\n",
    "    \n",
    "    return difference_values, weight_strengths\n",
    "\n",
    "\n",
    "def get_permuted_difference_distribution (post_units, pre_units, mode, iterations, weight_strengths, bin_edges):\n",
    "    # Sample from underlying group orientation distribution to ensure comparisons are fair\n",
    "    post_unit_orientations = np.array([u[\"orientation\"] for u in post_units])\n",
    "    pre_unit_orientations = np.array([u[\"orientation\"] for u in pre_units])\n",
    "    \n",
    "    binned_difference_values = []\n",
    "    \n",
    "    max_value, mod_value = (90, 180) if mode == 'orientation' else (180, 360)\n",
    "    selectivity_index = 'OSI' if mode == 'orientation' else 'DSI'\n",
    "    selectivity_thresh = OSI_THRESH if mode == 'orientation' else DSI_THRESH\n",
    "\n",
    "    for i in range(iterations):\n",
    "        a1 = np.random.choice(pre_unit_orientations, size=len(weight_strengths))\n",
    "        a2 = np.random.choice(post_unit_orientations, size=len(weight_strengths))\n",
    "        \n",
    "        difference_values = mod_value - abs(abs((a1 - a2)%max_value) - mod_value);\n",
    "\n",
    "        hist, edges = np.histogram(\n",
    "            difference_values,\n",
    "            bins=bin_edges,\n",
    "            weights=np.abs(weight_strengths)\n",
    "        )     \n",
    "        \n",
    "        binned_difference_values.append(hist)\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print('Iteration', i)\n",
    "\n",
    "    binned_difference_values = np.array(binned_difference_values)\n",
    "            \n",
    "    upper = np.percentile(binned_difference_values, 97.5, axis=0)\n",
    "    lower = np.percentile(binned_difference_values, 2.5, axis=0)\n",
    "        \n",
    "    cis = [(lower[i], upper[i]) for i in range(len(bin_edges)-1)]\n",
    "    \n",
    "    return binned_difference_values, cis\n",
    "\n",
    "def plot_orientation_cis (ax, cis):\n",
    "    for i, ci in enumerate(cis):\n",
    "        height = ci[1]-ci[0]\n",
    "        ax.add_patch(Rectangle(\n",
    "            (-0.4 + i, ci[0]),\n",
    "            0.8, height,\n",
    "            facecolor='green', alpha=0.75\n",
    "        ))\n",
    "        \n",
    "# Plot histogram\n",
    "physiology_data = [physiology_data_group1, physiology_data_group2]\n",
    "bin_edges = [[0, 30, 60, 90], [0, 36, 72, 108, 144, 180]]\n",
    "bin_edge_labels = [[\"0\", \"45\", \"90\"], [\"0\", \"45\", \"90\", \"135\", \"180\"]]\n",
    "\n",
    "for mode_i, mode in enumerate([\"orientation\", \"direction\"]):\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(8,6))\n",
    "\n",
    "    for pre_unit_i, pre_units in enumerate(physiology_data):    \n",
    "        for post_unit_i, post_units in enumerate(physiology_data):   \n",
    "            ax = axs[pre_unit_i, post_unit_i]\n",
    "\n",
    "            difference_values, weight_strengths = get_difference_values(\n",
    "                post_units,\n",
    "                pre_units,\n",
    "                mode\n",
    "            )\n",
    "            hist, edges = np.histogram(\n",
    "                difference_values,\n",
    "                bins=bin_edges[mode_i],\n",
    "                weights=np.abs(weight_strengths)\n",
    "            )\n",
    "            \n",
    "            null_distribution, cis = get_permuted_difference_distribution(\n",
    "                post_units,\n",
    "                pre_units,\n",
    "                mode,\n",
    "                iterations=10000,\n",
    "                weight_strengths=np.abs(weight_strengths),\n",
    "                bin_edges=bin_edges[mode_i]\n",
    "            )\n",
    "            \n",
    "            pvalues = []\n",
    "            for i in range(len(edges)-1):\n",
    "                p, _, _ = get_two_tailed_p_value(null_distribution[:, i], hist[i])\n",
    "                pvalues.append(p)\n",
    "\n",
    "            ax.bar(bin_edge_labels[mode_i], hist, color=NEUTRAL_COLOR)\n",
    "            plot_orientation_cis(ax, cis)\n",
    "            ax.set_title(\"Group {} - group {} ({})\".format(pre_unit_i+1, post_unit_i+1, pvalues))\n",
    "            ax.set_xlabel(\"Δ {} (degs)\".format(mode))\n",
    "            ax.set_ylabel(\"Weighted frequency\")\n",
    "\n",
    "    plt.suptitle(\"Δ {}\".format(mode))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_thresh = 2\n",
    "\n",
    "max_mean_response_1 = max(u['mean_response'] for u in physiology_data_group1)\n",
    "max_mean_response_2 = max(u['mean_response'] for u in physiology_data_group2)\n",
    "\n",
    "f_values_1 = []\n",
    "for unit in physiology_data_group1:\n",
    "    f = unit[\"f\"]\n",
    "    if f != False and unit[\"mean_response\"] > (0.01*max_mean_response_1):\n",
    "        f_values_1.append(f)\n",
    "f_values_1 = np.array(f_values_1)\n",
    "complex_cell_prop_1 = round(len(f_values_1[f_values_1 <= 1]) / len(f_values_1) * 100, 2)\n",
    "    \n",
    "f_values_2 = []\n",
    "for unit in physiology_data_group2:\n",
    "    f = unit[\"f\"]\n",
    "    if f != False and unit[\"mean_response\"] > (0.01*max_mean_response_2):\n",
    "        f_values_2.append(f)\n",
    "f_values_2 = np.array(f_values_2)\n",
    "complex_cell_prop_2 = round(len(f_values_2[f_values_2 <= 1]) / len(f_values_2) * 100, 2)\n",
    "\n",
    "bins = np.arange(0, outlier_thresh+0.1, 0.1)\n",
    "xlabels = np.round(bins, 1).astype(str)\n",
    "xlabels[-1] += '+'\n",
    "nlabels = len(xlabels)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=plt.figaspect(1/3), sharex=True)\n",
    "axs[0].hist(np.clip(f_values_1, bins[0], bins[-1]), bins=bins, color=GROUP1_COLOR)\n",
    "axs[0].set_title(\"Group 1 units ({}% complex cell)\".format(complex_cell_prop_1))\n",
    "axs[0].set_xlabel(\"F value\")\n",
    "#axs[0].set_xticks(bins)\n",
    "#axs[0].set_xticklabels(xlabels)\n",
    "#axs[0].set_xlim([0, outlier_thresh])\n",
    "\n",
    "axs[1].hist(np.clip(f_values_2, bins[0], bins[-1]), bins=bins, color=GROUP2_COLOR)\n",
    "axs[1].set_title(\"Group 2 units ({}% complex cell)\".format(complex_cell_prop_2))\n",
    "axs[1].set_xlabel(\"F value\")\n",
    "fig.suptitle('F values (> 1 indicates a simple cell)')\n",
    "\n",
    "f_values_1_complex = len(f_values_1[f_values_1 <= 1])\n",
    "f_values_2_complex = len(f_values_2[f_values_2 <= 1])\n",
    "f_values_1_simple = len(f_values_1[f_values_1 > 1])\n",
    "f_values_2_simple = len(f_values_2[f_values_2 > 1])\n",
    "\n",
    "chi_sq = np.array([\n",
    "    [f_values_1_complex, f_values_2_complex],\n",
    "    [f_values_1_simple, f_values_2_simple]\n",
    "])\n",
    "\n",
    "scipy.stats.chisquare(chi_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_env",
   "language": "python",
   "name": "rnn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
